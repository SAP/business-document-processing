{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification demo\n",
    "\n",
    "In this notebook we will learn how to use the **Document Classification** service using a python based client library.\n",
    "\n",
    "In this usage example we will use a pretrained model, which was trained to classify documents into the classes **invoice, purchase order and payment advice**. We will use a set of **fake invoices, purchase orders and payment advices which were generated for this workshop** to inspect the responses and judge the performance of the model.\n",
    "\n",
    "### Demo content\n",
    "\n",
    "1. Inference with a pretrained model\n",
    "1. Performance metrics of a classification model\n",
    "1. **Optional:** Use confidence values to sort out documents from \"Other\" category\n",
    "\n",
    "### Initial steps\n",
    "\n",
    "**Note:** The following steps are required for the execution of this notebook:\n",
    "\n",
    "1. See this [tutorial](https://developers.sap.com/tutorials/hcp-create-trial-account.html) to learn how to create a **SAP Cloud Platform Trial account**.\n",
    "2. See this [tutorial](https://developers.sap.com/tutorials/cp-aibus-dc-service-instance.html) to learn how to create a **Document Classification service instance**.\n",
    "\n",
    "Please keep the **service key** for the Document Classification service created in the 2nd step above at hand as we will use it below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Installation and invocation of the client library\n",
    "\n",
    "In this section we will:\n",
    "- Pass the service key we obtained in the **initial steps above** to the client library which will use it to communicate with the [Document Classification REST API](https://aiservices-trial-dc.cfapps.eu10.hana.ondemand.com/document-classification/v1/)\n",
    "- Import the client library package and create an instance of the client\n",
    "- Try out and learn about the [API of the client library](https://github.com/SAP/document-classification-client/blob/master/API.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the credentials to the client library\n",
    "\n",
    "# Please copy the content of the service key you created in the Prerequisites above here after \"service_key = \"\n",
    "# An EXAMPLE is given to show how the service key needs to be pasted here\n",
    "service_key = {\n",
    "  \"url\": \"https://environment.eu10.hana.ondemand.com\",\n",
    "  \"uaa\": {\n",
    "    \"tenantmode\": \"dedicated\",\n",
    "    \"sburl\": \"*******\",\n",
    "    \"subaccountid\": \"*******\",\n",
    "    \"clientid\": \"*******\",\n",
    "    \"xsappname\": \"*******\",\n",
    "    \"clientsecret\": \"*******\",\n",
    "    \"url\": \"*******\",\n",
    "    \"uaadomain\": \"*******\",\n",
    "    \"verificationkey\": \"*******\",\n",
    "    \"apiurl\": \"*******\",\n",
    "    \"identityzone\": \"*******\",\n",
    "    \"identityzoneid\": \"*******\",\n",
    "    \"tenantid\": \"*******\",\n",
    "    \"zoneid\": \"*******\"\n",
    "  },\n",
    "  \"swagger\": \"/document-classification/v1\"\n",
    "}\n",
    "\n",
    "# These 4 values are needed for communicating with the Document Classification REST API\n",
    "api_url = service_key[\"url\"]\n",
    "uaa_server = service_key[\"uaa\"][\"url\"]\n",
    "client_id = service_key[\"uaa\"][\"clientid\"]\n",
    "client_secret = service_key[\"uaa\"][\"clientsecret\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import client class from document classification client python package\n",
    "from sap_document_classification_client.dc_api_client import DCApiClient\n",
    "# Create an instance of this class with the credentials defined in the last cell\n",
    "my_dc_client = DCApiClient(api_url, client_id, client_secret, uaa_server)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Learn about different methods offered by the client library\n",
    "\n",
    "- Try using tab completion (type `my_dc_client.` and then hit the tab key to explore methods on the client library instance)\n",
    "- OR TRY the `?command` syntax used above to explore the possibilities availible to you via the client library!\n",
    "- You can also list all fields of the client instance by using `dir(my_dc_client)`\n",
    "- Try calling some methods, e.g. try to classify a document by running `my_dc_client.classify_document()` \n",
    "- Note that some functionality of the service is disabled in the trial landscape. This will be indicated by the error message:  `429 Client Error: Too Many Requests for url`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get more information about a given method\n",
    "?my_dc_client.classify_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify a document and show the response from the DC REST API\n",
    "response = my_dc_client.classify_document('data/classification_data/Invoice/norsk_folkehjelp.pdf', 'DocumentInfoRecord/AUT', '1')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out the GET method for viewing all inference documents in your tenant\n",
    "my_dc_client.get_classification_documents_info('DocumentInfoRecord/AUT', '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to delete the document, inference documents and their results have a time to live and will be clean up afterward\n",
    "# Make sure to get your results before that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Inference with a pretrained model\n",
    "\n",
    "In this section we will:\n",
    "\n",
    "- Locate the example documents provided for this workshop\n",
    "- **Classify those documents** using a pretrained model by sending them to the Document Classification REST API\n",
    "- Investigate and understand the **classification results** returned by the REST API as well as have a look at the respective documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all pdf in the data/classification subfolder of this repository recursively\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "filenames = glob(os.path.join(\"data/classification_data\", \"**\", \"*.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which files we found\n",
    "# Looks like we have 10 documents each in 4 subfolders which we will use as classes in this use case!\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example shows the help for the method we will use for running inference on a batch of documents using a given model\n",
    "?my_dc_client.classify_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method will send the documents to the Document Classifcation API and return the results provided by the pretrained model\n",
    "# Note that this can take up to 5 minutes to complete as the Document Classification service has process all 40 documents which involves:\n",
    "# Running Optical character recognition on the document & running the pretrained model on the extracted words\n",
    "results = my_dc_client.classify_documents(filenames, \"DocumentInfoRecord/AUT\", \"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Learn about the format of the classification results returned by the client library\n",
    "\n",
    "- Have a look at some documents and the classification results below (you can just modify the index and execute cells multiple times)\n",
    "- Try to understand the json returned by the Document Classification REST API which is stored in `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you can specify a document you would like to inspect in detail in the next 4 cells\n",
    "# The index is referring to the list of documents we printed 3 cells above\n",
    "document_index = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what this document looks like\n",
    "from IPython.display import IFrame\n",
    "IFrame(filenames[document_index], width=600, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the response we got back for the first document\n",
    "results[document_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This small loop will extract the important values in the response jsons and create vectors which are easier in the rest of the notebook\n",
    "# predictions variable will contain class predicted with highest confidence\n",
    "# confidences variable will contain confidence assigned by model for each class (will sum to 1 on each document as it was defined as multi-class)\n",
    "import numpy as np\n",
    "\n",
    "predictions = [] \n",
    "confidences = []\n",
    "for result in results:\n",
    "    temp = {}\n",
    "    for label in result[\"predictions\"][0][\"results\"]:\n",
    "        temp[label['label']] = label['score']\n",
    "    predictions.append(list(temp.keys())[np.argmax(list(temp.values()))])\n",
    "    confidences.append(temp)\n",
    "    \n",
    "# We also store the ground truth for each document based on folder it was contained in\n",
    "labels = list(map(lambda x: x.split(\"/\")[-2], filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most intersting part here are the classification results for the first and only characteristic this model was trained on\n",
    "# These results are contained in this field of the resposne\n",
    "# Each number gives a probabilty the pretrained model assigned for this document belonging to the respective class\n",
    "print(\"Predicted class: {}\".format(predictions[document_index]))\n",
    "print(\"Predicted confidences: {}\".format(confidences[document_index]))\n",
    "print(\"Ground truth:: {}\".format(labels[document_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the response to this document as bar graph \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1,1, figsize=(10, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "ax.bar(confidences[document_index].keys(), confidences[document_index].values())\n",
    "ax.set_ylabel(\"Confidence [%]\")\n",
    "ax.set_title(\"The correct solution is \\\"{}\\\"\".format(labels[document_index]))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Performance metrics of a classification model\n",
    "\n",
    "In this section we will:\n",
    "\n",
    "- Extract the **ground truth** from the folder hierachy our documents were already sorted into prior to this workshop\n",
    "- Learn about performance metrics for a classification model including the concepts of **type 2 errors**, **precision and recall**\n",
    "- Learn about two important visualizations to judge performance of a classifier: **confusion matrices** and **precision recall curves**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurences of labels in our ground truth vector\n",
    "from collections import Counter\n",
    "label_counts = Counter(labels)\n",
    "unique_labels = sorted(list(label_counts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distibution of the labels (i.e. whether we have a well-balanced test set here)\n",
    "fig, ax = plt.subplots(1,1, figsize=(10, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "ax.bar(label_counts.keys(), label_counts.values())\n",
    "ax.set_ylabel(\"Document count in class\")\n",
    "ax.set_title(\"This dataset seems to be well balanced! 10 documents from each class\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we will visualize the type 2 error matrix also known as confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "font = {'size': 22}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "def plot_confusion_matrix(ax, y_true, y_pred, classes, normalize=False, cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\" \n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "\n",
    "        # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           ylabel=\"True label\",\n",
    "           xlabel=\"Predicted label\",\n",
    "           xlim=(-0.5,len(classes)-0.5),\n",
    "           ylim=(-0.5,len(classes)-0.5))\n",
    "\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right',\n",
    "             rotation_mode='anchor')\n",
    "\n",
    "    # Loop over data dimensions and create text annotations (number in each rectangle)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha='center', va='center', color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "# In this figure we can see that no document of the \"Other\" category was predicted as such -> 9 were classified as PA, 1 as PO\n",
    "# This is expected as we did not train our model to predict this category (a update we might add soon, stay tuned!)\n",
    "# From the other 30 documents 2 were missclassified yielding an accuracy of 93.3% for these classes\n",
    "# The two missclassifcations in this category should be investigated below\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,10))\n",
    "plot_confusion_matrix(ax, labels, predictions, unique_labels, normalize=False)\n",
    "plt.title(\"Confusion matrix without normalization for characteristic document type\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize PR curve for each class in the one characteristic the model predicts\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def plot_f_score(ax):\n",
    "    f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "    for f_score in f_scores:\n",
    "        x = np.linspace(0.01, 1)\n",
    "        y = f_score * x / (2 * x - f_score)\n",
    "        l, = ax.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "        ax.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,10))\n",
    "for label in unique_labels:\n",
    "    if not label == \"Other\":\n",
    "        gt = [subelement == label for subelement in labels]\n",
    "        confidence = [subelement[label] for subelement in confidences]\n",
    "        precision, recall, thresholds = precision_recall_curve(gt, confidence)\n",
    "        ax.plot(recall, precision, label=label)\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_xlim(-0.1,1.1)\n",
    "ax.set_ylim(-0.1,1.1)\n",
    "ax.set_title('{}: Precision-Recall curves'.format(label))   \n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.get_xaxis().tick_bottom()\n",
    "ax.get_yaxis().tick_left()\n",
    "ax.legend(loc='lower left')\n",
    "ax.grid()\n",
    "plot_f_score(ax)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Understand the classification errors\n",
    "\n",
    "- Determine which documents where misclassified using the vectors `labels` and `predictions` (ignore \"Other\" category)\n",
    "- Display these documents and compare with other documents of that ground truth class, can you come up with a distinguishing feature?\n",
    "- OR: Plot a confusion matrix without the documents from the other category\n",
    "- OR: Plot the P/R curves without the documents from the other category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use confidence values to sort out documents from \"Other\" category\n",
    "\n",
    "In this section we will:\n",
    "\n",
    "- Visualize the confidences predicted for the different ground truth classes\n",
    "- Understand how we can use this visualization and thresholds on the predictions to modify model to our requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We put the classification results into a pandas data frame to visualize the distribution of confidences w.r.t. ground truth classes\n",
    "import pandas as pd\n",
    "data = pd.DataFrame({'Ground Truth': labels,\n",
    "                     'Invoice': map(lambda x:x[\"Invoice\"], confidences),\n",
    "                     'Payment Advice': map(lambda x:x[\"Payment Advice\"], confidences),\n",
    "                     'Purchase Order': map(lambda x:x[\"Purchase Order\"], confidences),})\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,10))\n",
    "pd.plotting.parallel_coordinates(data, 'Ground Truth', colormap=plt.get_cmap(\"tab10\"))\n",
    "plt.legend(loc='upper left')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define thresholds to sort out documents belonging to the \"Other\" category\n",
    "others_heuristic = [0.4 < elem[\"Payment Advice\"] < 0.6 and elem[\"Purchase Order\"] > 0.2 for elem in confidences]\n",
    "others_ground_truth =  [elem == \"Other\" for elem in labels]\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,10))\n",
    "plot_confusion_matrix(ax, others_ground_truth, others_heuristic, [False, True], normalize=False)\n",
    "plt.title(\"Confusion matrix without normalization for characteristic document type\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Understand the confidence scores\n",
    "\n",
    "- How can we improve the heuristic thresholds (5 our of 40 wrong in example)? Can we use a small scikit model for example?\n",
    "- Can you plot the confusion matrix of the whole classification task if we assume this heuristic would be applied as post processing step\n",
    "- Start a discussion on which potential issues this approach could have? What would be approaches to make it more sound statistically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "Further information about the product can be found here:\n",
    "\n",
    "- [SAP Help](https://help.sap.com/dc)\n",
    "- [Python Client library](https://github.com/SAP/document-classification-client)\n",
    "- [Developer Tutorials](https://developers.sap.com/tutorial-navigator.html?tag=products:technology-platform/sap-ai-business-services/document-classification)\n",
    "\n",
    "**Note:** This notebook covers inference and analysis of the responses of a pretrained model provided by the service itself.  \n",
    "Usually the different documents encountered in different settings and/or the requirement of defining custom classes one wants to predict, require training of ones own custom model. Under the **Developer Tutorials** link above a tutorial on how to **train a custom Document Classification model** can be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE END, in case you reached here you can play around and wait for the others to catch up"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
