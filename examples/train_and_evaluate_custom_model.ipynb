{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification Demo\n",
    "\n",
    "This notebook is designed to demonstrate the ease of use of the SAP AI Business Service Document Classification for classification tasks. In this demo we are training a model for classification and evaluate its performance.\n",
    "\n",
    "For the demo, we are using a Jupyter Notebook and make use of this client library to invoke the most important functions of the DC REST API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "This notebook requires a datset to train a model on. You need to provide this dataset in the folder defined as `dataset_folder` in the 2nd cell of this notebook below.\n",
    "\n",
    "The Document Classification Service Python Client accepts datasets as a folder with a pair of files ending in .pdf and .json for each document to be used for training. The .pdf file is the original document while the .json file specifies the ground truth annotation.\n",
    "\n",
    "An example of the format of the .json ground truth file expected by the service is:\n",
    "```\n",
    "{\n",
    "\"classification\": [\n",
    "    {\n",
    "    \"characteristic\": \"color\",\n",
    "    \"value\": \"red\"\n",
    "    },\n",
    "    {\n",
    "    \"characteristic\": \"size\",\n",
    "    \"value\": \"big\"\n",
    "    }\n",
    "]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sap-document-classification-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T04:41:04.535541Z",
     "start_time": "2019-11-09T04:41:04.531191Z"
    }
   },
   "outputs": [],
   "source": [
    "# Environment specific configuration\n",
    "api_url = \"\"\n",
    "uaa_server = \"\"\n",
    "client_id = \"\"\n",
    "client_secret = \"\"\n",
    "\n",
    "# Model specific configuration\n",
    "model_name = \"\"\n",
    "dataset_folder = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T04:41:09.713900Z",
     "start_time": "2019-11-09T04:41:08.132422Z"
    }
   },
   "outputs": [],
   "source": [
    "# import DC client library\n",
    "from sap_document_classification_client import dc_api_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T04:35:31.032608Z",
     "start_time": "2019-11-09T04:35:31.018851Z"
    }
   },
   "outputs": [],
   "source": [
    "# Obtain the dc client api handler \n",
    "my_dc_client = dc_api_client.DCApiClient(api_url, client_id, client_secret, uaa_server)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display access token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T03:52:13.722992Z",
     "start_time": "2019-11-09T03:52:13.391937Z"
    }
   },
   "outputs": [],
   "source": [
    "# Token can be used to interact with e.g. swagger UI to explore DC API\n",
    "print(my_dc_client.session.headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset for training of a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T03:53:04.220110Z",
     "start_time": "2019-11-09T03:53:03.806452Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create Training dataset\n",
    "response = my_dc_client.create_dataset()\n",
    "training_dataset_id = response[\"datasetId\"]\n",
    "print(\"Dataset created with datasetId: {}\".format(training_dataset_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T09:26:01.068519Z",
     "start_time": "2019-09-04T09:23:45.230188Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Upload training documents to the dataset from training directory\n",
    "print(\"Uploading training documents to the dataset\")\n",
    "my_dc_client.upload_documents_directory_to_dataset(training_dataset_id, dataset_folder)\n",
    "print(\"Finished uploading training documents to the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T09:36:49.651509Z",
     "start_time": "2019-09-04T09:36:49.370143Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pretty print the dataset statistics\n",
    "from pprint import pprint\n",
    "print(\"Dataset statistics\")\n",
    "dataset_stats = my_dc_client.get_dataset_info(training_dataset_id)\n",
    "pprint(dataset_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of label distribution\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nrCharacteristics = len(dataset_stats[\"groundTruths\"])\n",
    "fig, (ax) = plt.subplots(nrCharacteristics,1, figsize=(10, 15), dpi=80, facecolor='w', edgecolor='k')\n",
    "if nrCharacteristics==1:\n",
    "    ax = np.array((ax,)) \n",
    "for i in range(nrCharacteristics):\n",
    "    keys = [element[\"value\"] for element in  dataset_stats[\"groundTruths\"][i][\"classes\"]]\n",
    "    total = [element[\"total\"] for element in  dataset_stats[\"groundTruths\"][i][\"classes\"]]\n",
    "    ax[i].set_ylabel(\"Absolute\")\n",
    "    ax[i].bar(keys, total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T09:49:52.977862Z",
     "start_time": "2019-09-04T09:49:52.633337Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Start training job from model with modelName {}\".format(model_name))\n",
    "response = my_dc_client.train_model(model_name, training_dataset_id)\n",
    "pprint(response)\n",
    "print(\"Model training finished with status: {}\".format(response.get(\"status\")))\n",
    "if response.get(\"status\") == \"SUCCEEDED\":\n",
    "    model_version = response.get(\"modelVersion\")\n",
    "    print(\"Trained model: {}\".format(model_name))\n",
    "    print(\"Trained model version: {}\".format(model_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T10:01:14.801991Z",
     "start_time": "2019-09-04T10:01:14.229766Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check training statistics\n",
    "reponse = my_dc_client.get_trained_model_info(model_name, model_version)\n",
    "training_details = response.pop(\"details\")\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy model\n",
    "response = my_dc_client.deploy_model(model_name, model_version)\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test usage of the model by classifying a few documents and collecting results and ground truth\n",
    "import binascii\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "filenames = my_dc_client._find_files(dataset_folder, \"*.PDF\")\n",
    "test_filenames = []\n",
    "for filename in filenames:\n",
    "    # Check whether it is a test document\n",
    "    with open(filename, 'rb') as pdf_file:\n",
    "        is_test_document = (int(str(binascii.crc32(pdf_file.read()))) % 100) in range(90,100)\n",
    "    if is_test_document:\n",
    "        test_filenames.append(filename)\n",
    "\n",
    "# Classify all test documents\n",
    "responses = my_dc_client.classify_documents(test_filenames, model_name, model_version)\n",
    "\n",
    "# Iterate over responses and store results in convenient format\n",
    "test_prediction = defaultdict(lambda : [])\n",
    "test_probability = defaultdict(lambda : defaultdict(lambda : []))\n",
    "test_ground_truth = defaultdict(lambda : [])\n",
    "for response, filename in zip(responses, test_filenames):\n",
    "    pprint(response)\n",
    "    try:\n",
    "        # Parse response from DC service\n",
    "        prediction = response[\"predictions\"]\n",
    "        for element in prediction:\n",
    "            labels = []\n",
    "            scores = []\n",
    "            for subelement in element[\"results\"]:\n",
    "                labels.append(subelement[\"label\"])\n",
    "                scores.append(subelement[\"score\"])\n",
    "                test_probability[element[\"characteristic\"]][subelement[\"label\"]].append(subelement[\"score\"])\n",
    "            test_prediction[element[\"characteristic\"]].append(labels[np.argmax(np.asarray(scores))])\n",
    "        # Collect ground truth of all test documents\n",
    "        with open(filename.replace(\".pdf\", \".json\")) as gt_file:\n",
    "            gt = json.load(gt_file)\n",
    "        for element in gt[\"classification\"]:\n",
    "            test_ground_truth[element[\"characteristic\"]].append(element[\"value\"])\n",
    "    except KeyError:\n",
    "        print(\"Document not used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the ground truth and classification result for a certain document with index idx\n",
    "idx = 0\n",
    "\n",
    "for i in range(nrCharacteristics):\n",
    "    characteristic =dataset_stats[\"groundTruths\"][i][\"characteristic\"]\n",
    "    print(\"Ground truth for characteristic '{}'\".format(str(characteristic)) + \": '{}'\".format(test_ground_truth[str(characteristic)][idx]))\n",
    "\n",
    "print(\"Model predictions:\")\n",
    "pprint(responses[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find thresholds to avoid all false classifications in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These thresholds can be used for example to define when manual annotation (and ideally feedback into training processes) takes place\n",
    "thresholds = defaultdict(lambda : defaultdict(lambda : 0))\n",
    "is_wrong_classification = defaultdict(lambda : [])\n",
    "\n",
    "for characteristic in test_ground_truth.keys():\n",
    "    unique_labels = np.unique(np.asarray(test_ground_truth[characteristic]))\n",
    "    for label in unique_labels:\n",
    "        # This loop is only necessary if all documents are classified correctly to set thresholds to 1\n",
    "        thresholds[characteristic][label] = 1\n",
    "    for idx in range(len(test_ground_truth[characteristic])):\n",
    "        predicted_label = test_prediction[characteristic][idx]\n",
    "        is_wrong_classification[characteristic].append(test_prediction[characteristic][idx] != test_ground_truth[characteristic][idx])\n",
    "        if is_wrong_classification[characteristic][idx]:\n",
    "            if(thresholds[characteristic][predicted_label] > test_probability[characteristic][predicted_label][idx]):\n",
    "                thresholds[characteristic][predicted_label] = test_probability[characteristic][predicted_label][idx]\n",
    "                \n",
    "for characteristic in test_ground_truth.keys():\n",
    "    print(characteristic)\n",
    "    print(\"{} of {} documents classified wrong\".format(sum(is_wrong_classification[characteristic]), len(test_ground_truth[characteristic])))\n",
    "    pprint({ k:v for k,v in thresholds[characteristic].items() })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "font = {'size'   : 22}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "def plot_confusion_matrix(ax, char, y_true, y_pred, classes,\n",
    "                          normalize=True,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = \"{}: Normalized confusion matrix\".format(char)\n",
    "        else:\n",
    "            title = \"{}: Confusion matrix without normalization\".format(char)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "\n",
    "        # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel=\"True label\",\n",
    "           xlabel=\"Predicted label\",\n",
    "           xlim=(-0.5,len(classes)-0.5),\n",
    "           ylim=(-0.5,len(classes)-0.5))\n",
    "\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right',\n",
    "             rotation_mode='anchor')\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha='center', va='center', color='white' if cm[i, j] > thresh else 'black')\n",
    "    fig.show()\n",
    "\n",
    "fig, ax = plt.subplots(len(test_ground_truth.keys()), 1, figsize=(14,28))\n",
    "if len(test_ground_truth.keys())==1:\n",
    "    ax = np.array((ax,)) \n",
    "for idx, characteristic in enumerate(test_ground_truth.keys()):\n",
    "    plot_confusion_matrix(ax[idx], characteristic,\n",
    "                          test_ground_truth[characteristic], \n",
    "                          test_prediction[characteristic], \n",
    "                          np.unique(np.asarray(test_ground_truth[characteristic])), \n",
    "                          normalize=False)\n",
    "fig.subplots_adjust(hspace=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision Recall curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize PR curve for each characteristic (NOTE this as a bit boring in this example, create a more challenging dataset for algorithm?)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def plot_f_score(ax):\n",
    "    f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "    for f_score in f_scores:\n",
    "        x = np.linspace(0.01, 1)\n",
    "        y = f_score * x / (2 * x - f_score)\n",
    "        l, = ax.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "        ax.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "fig, ax = plt.subplots(len(test_ground_truth.keys()), 1, figsize=(12, 24), dpi=80, facecolor='w', edgecolor='k')\n",
    "if len(test_ground_truth.keys())==1:\n",
    "    ax = np.array((ax,)) \n",
    "\n",
    "for idx, characteristic in enumerate(test_ground_truth.keys()):\n",
    "    for label in np.unique(np.asarray(test_ground_truth[characteristic])):\n",
    "        gt = [subelement == label for subelement in test_ground_truth[characteristic]]\n",
    "        prediction = test_probability[characteristic][label]\n",
    "        precision, recall, thresholds = precision_recall_curve(gt, prediction)\n",
    "        ax[idx].plot(recall, precision, label=label)\n",
    "    ax[idx].set_xlabel('Recall')\n",
    "    ax[idx].set_ylabel('Precision')\n",
    "    ax[idx].set_xlim(-0.1,1.1)\n",
    "    ax[idx].set_ylim(-0.1,1.1)\n",
    "    ax[idx].set_title('{}: Precision-Recall curves'.format(characteristic))   \n",
    "    ax[idx].spines[\"top\"].set_visible(False)\n",
    "    ax[idx].spines[\"right\"].set_visible(False)\n",
    "    ax[idx].get_xaxis().tick_bottom()\n",
    "    ax[idx].get_yaxis().tick_left()\n",
    "    ax[idx].legend()\n",
    "    ax[idx].grid()\n",
    "    plot_f_score(ax[idx])\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
