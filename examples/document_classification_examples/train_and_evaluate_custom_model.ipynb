{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification Demo\n",
    "\n",
    "In this notebook we will learn how to use the **Document Classification** service using a Python-based client library. \n",
    "In this demo, we train a model for classification and evaluate its performance on a small example dataset.\n",
    "\n",
    "For the demo, we prepared this Jupyter Notebook which demonstrates the use of this client library to invoke the most important functions of the Document Classification REST API. \n",
    "\n",
    "### Demo content\n",
    "\n",
    "1. Create a dataset\n",
    "1. Train and deploy a custom model\n",
    "1. Run classification requests using the custom model\n",
    "\n",
    "### Initial steps\n",
    "\n",
    "**Note:** The following steps are required for the execution of this notebook (dataset creation and training/deployment features are not available on trial accounts):\n",
    "\n",
    "1. See this [tutorial](https://developers.sap.com/tutorials/hcp-create-trial-account.html) to learn how to create an **SAP Cloud Platform Trial account**.\n",
    "2. See this [tutorial](https://developers.sap.com/tutorials/cp-aibus-dc-service-instance.html) to learn how to create a **Document Classification service instance**.\n",
    "\n",
    "Please keep the **service key** for the Document Classification service created in the 2nd step above at hand as we will use it below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Installation and invocation of the client library\n",
    "\n",
    "In this section we will:\n",
    "- Pass the service key we obtained in the **initial steps above** to the client library which will use it to communicate with the [Document Classification REST API](https://aiservices-trial-dc.cfapps.eu10.hana.ondemand.com/document-classification/v1/)\n",
    "- Import the client library package and create an instance of the client\n",
    "- Try out and learn about the [API of the client library](https://github.com/SAP/business-document-processing/blob/main/API.md)\n",
    "\n",
    "An example dataset is provided in the repo, you can explore the structure of the dataset required [here](https://github.com/SAP/business-document-processing/tree/main/examples/document_classification_examples/data/training_data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T04:41:04.535541Z",
     "start_time": "2019-11-09T04:41:04.531191Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pass the credentials to the client library\n",
    "\n",
    "# Please copy the content of the service key you created in the Prerequisites above here after \"service_key = \"\n",
    "# An EXAMPLE is given to show how the service key needs to be pasted here\n",
    "service_key = {\n",
    "  \"url\": \"https://environment.eu10.hana.ondemand.com\",\n",
    "  \"uaa\": {\n",
    "    \"tenantmode\": \"dedicated\",\n",
    "    \"sburl\": \"*******\",\n",
    "    \"subaccountid\": \"*******\",\n",
    "    \"clientid\": \"*******\",\n",
    "    \"xsappname\": \"*******\",\n",
    "    \"clientsecret\": \"*******\",\n",
    "    \"url\": \"*******\",\n",
    "    \"uaadomain\": \"*******\",\n",
    "    \"verificationkey\": \"*******\",\n",
    "    \"apiurl\": \"*******\",\n",
    "    \"identityzone\": \"*******\",\n",
    "    \"identityzoneid\": \"*******\",\n",
    "    \"tenantid\": \"*******\",\n",
    "    \"zoneid\": \"*******\"\n",
    "  },\n",
    "  \"swagger\": \"/document-classification/v1\"\n",
    "}\n",
    "\n",
    "# These 4 values are needed for communicating with the Document Classification REST API\n",
    "api_url = service_key[\"url\"]\n",
    "uaa_server = service_key[\"uaa\"][\"url\"]\n",
    "client_id = service_key[\"uaa\"][\"clientid\"]\n",
    "client_secret = service_key[\"uaa\"][\"clientsecret\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning:</b> Please provide a model name for training.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model specific configuration\n",
    "model_name = \"\" # choose an arbitrary model name for the model trained here, will be assigned to the trained model for identification purposes\n",
    "dataset_folder = \"data/training_data/\" # should point to (relative or absolute) path containing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T04:35:31.032608Z",
     "start_time": "2019-11-09T04:35:31.018851Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import client class from document classification client python package\n",
    "from sap_business_document_processing import DCApiClient\n",
    "# Create an instance of this class with the credentials defined in the last cell\n",
    "my_dc_client = DCApiClient(api_url, client_id, client_secret, uaa_server)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset for training of a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T03:53:04.220110Z",
     "start_time": "2019-11-09T03:53:03.806452Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create Training dataset\n",
    "response = my_dc_client.create_dataset()\n",
    "training_dataset_id = response[\"datasetId\"]\n",
    "print(\"Dataset created with datasetId: {}\".format(training_dataset_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T09:26:01.068519Z",
     "start_time": "2019-09-04T09:23:45.230188Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Upload training documents to the dataset from training directory\n",
    "print(\"Uploading training documents to the dataset\")\n",
    "my_dc_client.upload_documents_directory_to_dataset(training_dataset_id, dataset_folder)\n",
    "print(\"Finished uploading training documents to the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T09:36:49.651509Z",
     "start_time": "2019-09-04T09:36:49.370143Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pretty print the dataset statistics\n",
    "from pprint import pprint\n",
    "print(\"Dataset statistics\")\n",
    "dataset_stats = my_dc_client.get_dataset_info(training_dataset_id)\n",
    "pprint(dataset_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of label distribution\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "nrCharacteristics = len(dataset_stats[\"groundTruths\"])\n",
    "fig, (ax) = plt.subplots(nrCharacteristics,1, figsize=(10, 15), dpi=80, facecolor='w', edgecolor='k')\n",
    "if nrCharacteristics==1:\n",
    "    ax = np.array((ax,)) \n",
    "for i in range(nrCharacteristics):\n",
    "    keys = [element[\"value\"] for element in  dataset_stats[\"groundTruths\"][i][\"classes\"]]\n",
    "    total = [element[\"total\"] for element in  dataset_stats[\"groundTruths\"][i][\"classes\"]]\n",
    "    ax[i].set_ylabel(\"Absolute\")\n",
    "    ax[i].bar(keys, total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T09:49:52.977862Z",
     "start_time": "2019-09-04T09:49:52.633337Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Start training job from model with modelName {}\".format(model_name))\n",
    "response = my_dc_client.train_model(model_name, training_dataset_id)\n",
    "pprint(response)\n",
    "print(\"Model training finished with status: {}\".format(response.get(\"status\")))\n",
    "if response.get(\"status\") == \"SUCCEEDED\":\n",
    "    model_version = response.get(\"modelVersion\")\n",
    "    print(\"Trained model: {}\".format(model_name))\n",
    "    print(\"Trained model version: {}\".format(model_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T10:01:14.801991Z",
     "start_time": "2019-09-04T10:01:14.229766Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check training statistics\n",
    "response = my_dc_client.get_trained_model_info(model_name, model_version)\n",
    "training_details = response.pop(\"details\")\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy model\n",
    "response = my_dc_client.deploy_model(model_name, model_version)\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "This runs inference on all documents in the test set (stratification is done inside DC service and reproduced here).  \n",
    "We are working on exposing the stratification results so that this cell can be shortened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test usage of the model by classifying a few documents and collecting results and ground truth\n",
    "import binascii\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "filenames = my_dc_client._find_files(dataset_folder, \"*.PDF\")\n",
    "test_filenames = []\n",
    "for filename in filenames:\n",
    "    # Check whether it is a test document\n",
    "    with open(filename, 'rb') as pdf_file:\n",
    "        is_test_document = (int(str(binascii.crc32(pdf_file.read()))) % 100) in range(90,100)\n",
    "    if is_test_document:\n",
    "        test_filenames.append(filename)\n",
    "\n",
    "# Classify all test documents\n",
    "responses = list(my_dc_client.classify_documents(test_filenames, model_name, model_version))\n",
    "\n",
    "# Iterate over responses and store results in convenient format\n",
    "test_prediction = defaultdict(lambda : [])\n",
    "test_probability = defaultdict(lambda : defaultdict(lambda : []))\n",
    "test_ground_truth = defaultdict(lambda : [])\n",
    "for response, filename in zip(responses, test_filenames):\n",
    "    pprint(response)\n",
    "    try:\n",
    "        # Parse response from DC service\n",
    "        prediction = response[\"predictions\"]\n",
    "        for element in prediction:\n",
    "            labels = []\n",
    "            scores = []\n",
    "            for subelement in element[\"results\"]:\n",
    "                labels.append(subelement[\"label\"])\n",
    "                scores.append(subelement[\"score\"])\n",
    "                test_probability[element[\"characteristic\"]][subelement[\"label\"]].append(subelement[\"score\"])\n",
    "            test_prediction[element[\"characteristic\"]].append(labels[np.argmax(np.asarray(scores))])\n",
    "        # Collect ground truth of all test documents\n",
    "        with open(filename.replace(\".pdf\", \".json\")) as gt_file:\n",
    "            gt = json.load(gt_file)\n",
    "        for element in gt[\"classification\"]:\n",
    "            test_ground_truth[element[\"characteristic\"]].append(element[\"value\"])\n",
    "    except KeyError:\n",
    "        print(\"Document not used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the ground truth and classification result for a certain document with index idx\n",
    "idx = 0\n",
    "\n",
    "for i in range(nrCharacteristics):\n",
    "    characteristic =dataset_stats[\"groundTruths\"][i][\"characteristic\"]\n",
    "    print(\"Ground truth for characteristic '{}'\".format(str(characteristic)) + \": '{}'\".format(test_ground_truth[str(characteristic)][idx]))\n",
    "\n",
    "print(\"Model predictions:\")\n",
    "pprint(responses[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "The confusion matrix is a way to visualize the results of a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "font = {'size'   : 22}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "def plot_confusion_matrix(ax, char, y_true, y_pred, classes,\n",
    "                          normalize=True,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = \"{}: Normalized confusion matrix\".format(char)\n",
    "        else:\n",
    "            title = \"{}: Confusion matrix without normalization\".format(char)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "\n",
    "        # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel=\"True label\",\n",
    "           xlabel=\"Predicted label\",\n",
    "           xlim=(-0.5,len(classes)-0.5),\n",
    "           ylim=(-0.5,len(classes)-0.5))\n",
    "\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right',\n",
    "             rotation_mode='anchor')\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha='center', va='center', color='white' if cm[i, j] > thresh else 'black')\n",
    "    fig.show()\n",
    "\n",
    "fig, ax = plt.subplots(len(test_ground_truth.keys()), 1, figsize=(14,28))\n",
    "if len(test_ground_truth.keys())==1:\n",
    "    ax = np.array((ax,)) \n",
    "for idx, characteristic in enumerate(test_ground_truth.keys()):\n",
    "    plot_confusion_matrix(ax[idx], characteristic,\n",
    "                          test_ground_truth[characteristic], \n",
    "                          test_prediction[characteristic], \n",
    "                          np.unique(np.asarray(test_ground_truth[characteristic])), \n",
    "                          normalize=False)\n",
    "fig.subplots_adjust(hspace=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision Recall curves\n",
    "\n",
    "Note that a Precision recall curve is only well defined for binary classification tasks.  \n",
    "For the multi-class case each class is treated independently based on the confidence scores provided by the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize PR curve for each characteristic (NOTE this as a bit boring in this example, create a more challenging dataset for algorithm?)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def plot_f_score(ax):\n",
    "    f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "    for f_score in f_scores:\n",
    "        x = np.linspace(0.01, 1)\n",
    "        y = f_score * x / (2 * x - f_score)\n",
    "        l, = ax.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "        ax.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "fig, ax = plt.subplots(len(test_ground_truth.keys()), 1, figsize=(12, 24), dpi=80, facecolor='w', edgecolor='k')\n",
    "if len(test_ground_truth.keys())==1:\n",
    "    ax = np.array((ax,)) \n",
    "\n",
    "for idx, characteristic in enumerate(test_ground_truth.keys()):\n",
    "    for label in np.unique(np.asarray(test_ground_truth[characteristic])):\n",
    "        gt = [subelement == label for subelement in test_ground_truth[characteristic]]\n",
    "        prediction = test_probability[characteristic][label]\n",
    "        precision, recall, thresholds = precision_recall_curve(gt, prediction)\n",
    "        ax[idx].plot(recall, precision, label=label)\n",
    "    ax[idx].set_xlabel('Recall')\n",
    "    ax[idx].set_ylabel('Precision')\n",
    "    ax[idx].set_xlim(-0.1,1.1)\n",
    "    ax[idx].set_ylim(-0.1,1.1)\n",
    "    ax[idx].set_title('{}: Precision-Recall curves'.format(characteristic))   \n",
    "    ax[idx].spines[\"top\"].set_visible(False)\n",
    "    ax[idx].spines[\"right\"].set_visible(False)\n",
    "    ax[idx].get_xaxis().tick_bottom()\n",
    "    ax[idx].get_yaxis().tick_left()\n",
    "    ax[idx].legend()\n",
    "    ax[idx].grid()\n",
    "    plot_f_score(ax[idx])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
